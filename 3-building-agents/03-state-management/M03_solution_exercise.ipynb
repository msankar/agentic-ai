{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9baf30a2",
   "metadata": {},
   "source": [
    "# [SOLUTION] Exercise - Building a Multi-Step State Machine Agent\n",
    "\n",
    "In this exercise, you will build an agent that manages a multi-step workflow using a state machine. You’ll define a custom state schema, implement step logic, connect steps (including conditional routing and loops), and run the workflow to process user input through several transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb9056",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You have learned how to use a state machine to manage workflow steps and transitions. Now, your challenge is to:\n",
    "\n",
    "- Define a state schema with multiple fields (e.g., user_query, instructions, messages, current_tool_calls).\n",
    "- Implement at least three step functions:\n",
    "    - Prepare Messages: Assemble the conversation history and any required context for the LLM.\n",
    "    - LLM: Call the language model to generate a response or tool call.\n",
    "    - Tools: Execute any required tool calls and update the state with results.\n",
    "- Connect steps to form a workflow, including:\n",
    "    - Entrypoint and Termination steps to start and end the workflow.\n",
    "    - Conditional routing: If the LLM response includes tool calls, route to the Tools step; otherwise, proceed to Termination.\n",
    "    - Looping: After executing tools, return to the LLM step to continue the workflow until there are no more tool calls.\n",
    "- Run your state machine with a sample input and inspect the state transitions and snapshots to understand how your agent processes a task step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0e231",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e42ccb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T18:26:29.405109Z",
     "start_time": "2025-11-05T18:26:29.391272Z"
    }
   },
   "source": [
    "from typing import TypedDict, List, Optional, Union\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from lib.state_machine import StateMachine, Step, EntryPoint, Termination, Run\n",
    "from lib.llm import LLM\n",
    "from lib.messages import AIMessage, UserMessage, SystemMessage, ToolMessage\n",
    "from lib.tooling import Tool, ToolCall, tool"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1a8686cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T18:26:30.160628Z",
     "start_time": "2025-11-05T18:26:30.155255Z"
    }
   },
   "source": [
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "b41de723",
   "metadata": {},
   "source": [
    "## Define a State Schema\n",
    "\n",
    "Create a TypedDict to represent the agent’s state, including fields for the user query, instructions, message history, and any pending tool calls."
   ]
  },
  {
   "cell_type": "code",
   "id": "99605c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T18:26:45.627757Z",
     "start_time": "2025-11-05T18:26:45.626036Z"
    }
   },
   "source": [
    "class AgentState(TypedDict):\n",
    "    user_query: str  # The current user query being processed\n",
    "    instructions: str  # System instructions for the agent\n",
    "    messages: List[dict]  # List of conversation messages\n",
    "    current_tool_calls: Optional[List[ToolCall]]  # Current pending tool calls"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "3da75109",
   "metadata": {},
   "source": [
    "## Define the Tools you will use\n",
    "\n",
    "Feel free to modify to add any tool you want"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1d171e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T18:26:47.000156Z",
     "start_time": "2025-11-05T18:26:46.996291Z"
    }
   },
   "source": [
    "@tool\n",
    "def get_games(num_games:int=1, top:bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Returns the top or bottom N games with highest or lowest scores.    \n",
    "    args:\n",
    "        num_games (int): Number of games to return (default is 1)\n",
    "        top (bool): If True, return top games, otherwise return bottom (default is True)\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98},\n",
    "        {\"Game\": \"Super Mario Odyssey\", \"Platform\": \"Switch\", \"Score\": 97},\n",
    "        {\"Game\": \"Metroid Prime\", \"Platform\": \"GameCube\", \"Score\": 97},\n",
    "        {\"Game\": \"Super Smash Bros. Brawl\", \"Platform\": \"Wii\", \"Score\": 93},\n",
    "        {\"Game\": \"Mario Kart 8 Deluxe\", \"Platform\": \"Switch\", \"Score\": 92},\n",
    "        {\"Game\": \"Fire Emblem: Awakening\", \"Platform\": \"3DS\", \"Score\": 92},\n",
    "        {\"Game\": \"Donkey Kong Country Returns\", \"Platform\": \"Wii\", \"Score\": 87},\n",
    "        {\"Game\": \"Luigi's Mansion 3\", \"Platform\": \"Switch\", \"Score\": 86},\n",
    "        {\"Game\": \"Pikmin 3\", \"Platform\": \"Wii U\", \"Score\": 85},\n",
    "        {\"Game\": \"Animal Crossing: New Leaf\", \"Platform\": \"3DS\", \"Score\": 88}\n",
    "    ]\n",
    "    # Sort the games list by Score\n",
    "    # If top is True, descending order\n",
    "    sorted_games = sorted(data, key=lambda x: x['Score'], reverse=top)\n",
    "    \n",
    "    # Return the N games\n",
    "    return sorted_games[:num_games]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "a07c2679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T18:26:47.684517Z",
     "start_time": "2025-11-05T18:26:47.682908Z"
    }
   },
   "source": [
    "tools = [get_games]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "052be6c2",
   "metadata": {},
   "source": [
    "## Create the Steps\n",
    "\n",
    "Write functions for each step in your workflow:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd86828",
   "metadata": {},
   "source": [
    "**Prepare Messages**: Build the message list for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "id": "6341e5c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:21.412824Z",
     "start_time": "2025-11-04T05:34:21.409183Z"
    }
   },
   "source": [
    "def prepare_messages_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Step logic: Prepare messages for LLM consumption\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=state[\"instructions\"]),\n",
    "        UserMessage(content=state[\"user_query\"])\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": messages\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "1e4ce1ea",
   "metadata": {},
   "source": [
    "**LLM Step**: Call the language model and check for tool calls."
   ]
  },
  {
   "cell_type": "code",
   "id": "0dbf3396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:24.429315Z",
     "start_time": "2025-11-04T05:34:24.426048Z"
    }
   },
   "source": [
    "def llm_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Step logic: Process the current state through the LLM\"\"\"\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = LLM(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    tool_calls = response.tool_calls if response.tool_calls else None\n",
    "\n",
    "    # Create AI message with content and tool calls\n",
    "    ai_message = AIMessage(content=response.content, tool_calls=tool_calls)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [ai_message],\n",
    "        \"current_tool_calls\": tool_calls\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "5288ecd4",
   "metadata": {},
   "source": [
    "**Tool Step**: Execute any tool calls and update the state."
   ]
  },
  {
   "cell_type": "code",
   "id": "32124e21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:33.907873Z",
     "start_time": "2025-11-04T05:34:33.904039Z"
    }
   },
   "source": [
    "def tool_step(state: AgentState) -> AgentState:\n",
    "    \"\"\"Step logic: Execute any pending tool calls\"\"\"\n",
    "    tool_calls = state[\"current_tool_calls\"] or []\n",
    "    tool_messages = []\n",
    "    \n",
    "    for call in tool_calls:\n",
    "        # Access tool call data correctly\n",
    "        function_name = call.function.name\n",
    "        function_args = json.loads(call.function.arguments)\n",
    "        tool_call_id = call.id\n",
    "        # Find the matching tool\n",
    "        tool = next((t for t in tools if t.name == function_name), None)\n",
    "        if tool:\n",
    "            result = tool(**function_args)\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(result), \n",
    "                    tool_call_id=tool_call_id, \n",
    "                    name=function_name, \n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Clear tool calls and add results to messages\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + tool_messages,\n",
    "        \"current_tool_calls\": None\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b6aaddee",
   "metadata": {},
   "source": [
    "## Build and Connect the State Machine\n",
    "\n",
    "Add your steps to the state machine, and connect them with transitions. Use conditional routing to decide whether to call tools or terminate, and loop as needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "58ca0974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:36.457533Z",
     "start_time": "2025-11-04T05:34:36.455004Z"
    }
   },
   "source": [
    "workflow = StateMachine[AgentState](AgentState)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "d52f8f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:38.061879Z",
     "start_time": "2025-11-04T05:34:38.058705Z"
    }
   },
   "source": [
    "# Create steps\n",
    "entry = EntryPoint[AgentState]()\n",
    "message_prep = Step[AgentState](\"message_prep\", prepare_messages_step)\n",
    "llm_processor = Step[AgentState](\"llm_processor\", llm_step)\n",
    "tool_executor = Step[AgentState](\"tool_executor\", tool_step)\n",
    "termination = Termination[AgentState]()\n",
    "        \n",
    "workflow.add_steps(\n",
    "    [\n",
    "        entry, \n",
    "        message_prep, \n",
    "        llm_processor, \n",
    "        tool_executor, \n",
    "        termination\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "7ae5a49c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:39.743992Z",
     "start_time": "2025-11-04T05:34:39.740280Z"
    }
   },
   "source": [
    "# Add transitions\n",
    "workflow.connect(entry, message_prep)\n",
    "workflow.connect(message_prep, llm_processor)\n",
    "\n",
    "# Transition based on whether there are tool calls\n",
    "def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:\n",
    "    \"\"\"Transition logic: Check if there are tool calls\"\"\"\n",
    "    if state.get(\"current_tool_calls\"):\n",
    "        return tool_executor\n",
    "    return termination\n",
    "\n",
    "# Routing: If tool calls -> tool_executor\n",
    "workflow.connect(\n",
    "    source=llm_processor, \n",
    "    targets=[tool_executor, termination], \n",
    "    condition=check_tool_calls\n",
    ")\n",
    "\n",
    "# Looping: Go back to llm after tool execution\n",
    "workflow.connect(\n",
    "    source=tool_executor, \n",
    "    targets=llm_processor\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "7ffeb8b6",
   "metadata": {},
   "source": [
    "## Run the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "id": "df4facbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:41.459566Z",
     "start_time": "2025-11-04T05:34:41.457444Z"
    }
   },
   "source": [
    "initial_state: AgentState = {\n",
    "    \"user_query\": \"What's the best game in the dataset?\",\n",
    "    \"instructions\": \"You can bring insights about a game dataset based on users questions\",\n",
    "    \"messages\": [],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8174c911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:35:35.376501Z",
     "start_time": "2025-11-04T05:35:35.090809Z"
    }
   },
   "source": [
    "run_object = workflow.run(initial_state)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: voc-1454**************************************6929. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAuthenticationError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m run_object = \u001B[43mworkflow\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/3-building-agents/03-state-management/lib/state_machine.py:206\u001B[39m, in \u001B[36mStateMachine.run\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m    203\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    205\u001B[39m \u001B[38;5;66;03m# Replace state entirely\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m state = \u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstate_schema\u001B[49m\u001B[43m)\u001B[49m  \n\u001B[32m    208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(step, EntryPoint):\n\u001B[32m    209\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[StateMachine] Starting: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_step_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/3-building-agents/03-state-management/lib/state_machine.py:22\u001B[39m, in \u001B[36mStep.run\u001B[39m\u001B[34m(self, state, state_schema)\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: StateSchema, state_schema: Type[StateSchema]) -> StateSchema:\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlogic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m     \u001B[38;5;66;03m# Get expected fields from the TypedDict\u001B[39;00m\n\u001B[32m     24\u001B[39m     expected_fields = get_type_hints(state_schema)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mllm_step\u001B[39m\u001B[34m(state)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Initialize LLM\u001B[39;00m\n\u001B[32m      5\u001B[39m llm = LLM(\n\u001B[32m      6\u001B[39m     model=\u001B[33m\"\u001B[39m\u001B[33mgpt-4o-mini\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     temperature=\u001B[32m0.3\u001B[39m,\n\u001B[32m      8\u001B[39m     tools=tools,\n\u001B[32m      9\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m response = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m tool_calls = response.tool_calls \u001B[38;5;28;01mif\u001B[39;00m response.tool_calls \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Create AI message with content and tool calls\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/3-building-agents/03-state-management/lib/llm.py:63\u001B[39m, in \u001B[36mLLM.invoke\u001B[39m\u001B[34m(self, input, response_format)\u001B[39m\n\u001B[32m     61\u001B[39m     response = \u001B[38;5;28mself\u001B[39m.client.beta.chat.completions.parse(**payload)\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     64\u001B[39m choice = response.choices[\u001B[32m0\u001B[39m]\n\u001B[32m     65\u001B[39m message = choice.message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    285\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    882\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    883\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    884\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    922\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = NOT_GIVEN,\n\u001B[32m    923\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m    924\u001B[39m     validate_response_format(response_format)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    938\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    939\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    940\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodalities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprediction\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweb_search_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/_base_client.py:1239\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1225\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1226\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1227\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1234\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1235\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1236\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1237\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1238\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1239\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/_base_client.py:1034\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1031\u001B[39m             err.response.read()\n\u001B[32m   1033\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1034\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1036\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mAuthenticationError\u001B[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: voc-1454**************************************6929. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "44f2c752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T05:34:47.081500Z",
     "start_time": "2025-11-04T05:34:47.031537Z"
    }
   },
   "source": [
    "run_object.get_final_state()[\"messages\"]"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_object' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrun_object\u001B[49m.get_final_state()[\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[31mNameError\u001B[39m: name 'run_object' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "89629e4a",
   "metadata": {},
   "source": [
    "## Optional \n",
    "\n",
    "Create an Agent class to encapsulate State Machine logic. Then try adding more tools, and experiment with different user queries to see how the workflow adapts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2e7d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 instructions: str, \n",
    "                 tools: List[Tool] = None,\n",
    "                 temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize an Agent instance\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name/identifier of the LLM model to use\n",
    "            instructions: System instructions for the agent\n",
    "            tools: Optional list of tools available to the agent\n",
    "            temperature: Temperature parameter for LLM (default: 0.7)\n",
    "        \"\"\"\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools if tools else []\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "                \n",
    "        # Initialize state machine\n",
    "        self.workflow = self._create_state_machine()\n",
    "\n",
    "    def _prepare_messages_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Prepare messages for LLM consumption\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=state[\"instructions\"]),\n",
    "            UserMessage(content=state[\"user_query\"])\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    def _llm_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Process the current state through the LLM\"\"\"\n",
    "\n",
    "        # Initialize LLM\n",
    "        llm = LLM(\n",
    "            model=self.model_name,\n",
    "            temperature=self.temperature,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        tool_calls = response.tool_calls if response.tool_calls else None\n",
    "\n",
    "        # Create AI message with content and tool calls\n",
    "        ai_message = AIMessage(content=response.content, tool_calls=tool_calls)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"current_tool_calls\": tool_calls\n",
    "        }\n",
    "\n",
    "    def _tool_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Execute any pending tool calls\"\"\"\n",
    "        tool_calls = state[\"current_tool_calls\"] or []\n",
    "        tool_messages = []\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            # Access tool call data correctly\n",
    "            function_name = call.function.name\n",
    "            function_args = json.loads(call.function.arguments)\n",
    "            tool_call_id = call.id\n",
    "            # Find the matching tool\n",
    "            tool = next((t for t in self.tools if t.name == function_name), None)\n",
    "            if tool:\n",
    "                result = tool(**function_args)\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=json.dumps(result), \n",
    "                        tool_call_id=tool_call_id, \n",
    "                        name=function_name, \n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Clear tool calls and add results to messages\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + tool_messages,\n",
    "            \"current_tool_calls\": None\n",
    "        }\n",
    "\n",
    "    def _create_state_machine(self) -> StateMachine[AgentState]:\n",
    "        \"\"\"Create the internal state machine for the agent\"\"\"\n",
    "        machine = StateMachine[AgentState](AgentState)\n",
    "        \n",
    "        # Create steps\n",
    "        entry = EntryPoint[AgentState]()\n",
    "        message_prep = Step[AgentState](\"message_prep\", self._prepare_messages_step)\n",
    "        llm_processor = Step[AgentState](\"llm_processor\", self._llm_step)\n",
    "        tool_executor = Step[AgentState](\"tool_executor\", self._tool_step)\n",
    "        termination = Termination[AgentState]()\n",
    "        \n",
    "        machine.add_steps([entry, message_prep, llm_processor, tool_executor, termination])\n",
    "        \n",
    "        # Add transitions\n",
    "        machine.connect(entry, message_prep)\n",
    "        machine.connect(message_prep, llm_processor)\n",
    "        \n",
    "        # Transition based on whether there are tool calls\n",
    "        def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:\n",
    "            \"\"\"Transition logic: Check if there are tool calls\"\"\"\n",
    "            if state.get(\"current_tool_calls\"):\n",
    "                return tool_executor\n",
    "            return termination\n",
    "        \n",
    "        machine.connect(llm_processor, [tool_executor, termination], check_tool_calls)\n",
    "        machine.connect(tool_executor, llm_processor)  # Go back to llm after tool execution\n",
    "        \n",
    "        return machine\n",
    "\n",
    "    def invoke(self, query: str) -> Run:\n",
    "        \"\"\"\n",
    "        Run the agent on a query\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to process\n",
    "            \n",
    "        Returns:\n",
    "            The final run object after processing\n",
    "        \"\"\"\n",
    "\n",
    "        initial_state: AgentState = {\n",
    "            \"user_query\": query,\n",
    "            \"instructions\": self.instructions,\n",
    "            \"messages\": [],\n",
    "        }\n",
    "\n",
    "        run_object = self.workflow.run(initial_state)\n",
    "\n",
    "        return run_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c738d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def power(base:float, exponent:float):\n",
    "    \"\"\"Exponentatiation: base to the power of exponent\"\"\"\n",
    "    \n",
    "    return base ** exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "887bda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(number_a:float, number_b:float):\n",
    "    \"\"\"Multiplication: number_a times number_b\"\"\"\n",
    "    \n",
    "    return number_a * number_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a1da1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [power, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "356eba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    instructions=(\n",
    "        \"You're an AI Agent very good with math operations \"\n",
    "        \"You can answer multistep questions by sequentially calling functions. \"\n",
    "        \"You follow a pattern of of Thought and Action. \"\n",
    "        \"Create a plan of execution: \"\n",
    "        \"- Use Thought to describe your thoughts about the question you have been asked. \"\n",
    "        \"- Use Action to specify one of the tools available to you. if you don't have a tool available, you can respond directly.\"\n",
    "        \"When you think it's over, return the answer \"\n",
    "        \"Never try to respond directly if the question needs a tool. \"\n",
    "        \"But if you don't have a tool available, you can respond directly. \"\n",
    "        f\"The actions you have are the Tools: {tools}. \\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84dffc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n"
     ]
    }
   ],
   "source": [
    "run_object = math_agent.invoke(\n",
    "    query=\"What's 3 to the power of 2? Take the result, then multiply it by 5.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d29595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(role='system', content=\"You're an AI Agent very good with math operations You can answer multistep questions by sequentially calling functions. You follow a pattern of of Thought and Action. Create a plan of execution: - Use Thought to describe your thoughts about the question you have been asked. - Use Action to specify one of the tools available to you. if you don't have a tool available, you can respond directly.When you think it's over, return the answer Never try to respond directly if the question needs a tool. But if you don't have a tool available, you can respond directly. The actions you have are the Tools: [<Tool name=power params=['base', 'exponent']>, <Tool name=multiply params=['number_a', 'number_b']>]. \\n\"),\n",
       " UserMessage(role='user', content=\"What's 3 to the power of 2? Take the result, then multiply it by 5.\"),\n",
       " AIMessage(role='assistant', content='Thought: First, I need to calculate \\\\(3\\\\) to the power of \\\\(2\\\\). Then, I will take that result and multiply it by \\\\(5\\\\). \\n\\nAction: I will use the power tool to calculate \\\\(3\\\\) to the power of \\\\(2\\\\).', tool_calls=[ChatCompletionMessageToolCall(id='call_KbjdkuHcTpzDAGuVis0dUfNz', function=Function(arguments='{\"base\":3,\"exponent\":2}', name='power'), type='function')]),\n",
       " ToolMessage(role='tool', content='9', tool_call_id='call_KbjdkuHcTpzDAGuVis0dUfNz', name='power'),\n",
       " AIMessage(role='assistant', content='Thought: The result of \\\\(3\\\\) to the power of \\\\(2\\\\) is \\\\(9\\\\). Now, I will multiply this result by \\\\(5\\\\).\\n\\nAction: I will use the multiply tool to calculate \\\\(9 \\\\times 5\\\\).', tool_calls=[ChatCompletionMessageToolCall(id='call_L7DRagsmJIPuSNB8vYdjWslN', function=Function(arguments='{\"number_a\":9,\"number_b\":5}', name='multiply'), type='function')]),\n",
       " ToolMessage(role='tool', content='45', tool_call_id='call_L7DRagsmJIPuSNB8vYdjWslN', name='multiply'),\n",
       " AIMessage(role='assistant', content='The final answer is \\\\(45\\\\).', tool_calls=None)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_object.get_final_state()[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceddaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
