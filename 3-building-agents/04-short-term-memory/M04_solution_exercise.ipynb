{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3778898",
   "metadata": {},
   "source": [
    "# [SOLUTION] Exercise - Building an Agent with Short-Term Memory\n",
    "\n",
    "In this exercise, you’ll extend your agent to support short-term memory across sessions. While state is used to manage the agent’s progress within a single run, memory allows your agent to remember what happened in previous runs, enabling context continuity across multiple user interactions.\n",
    "\n",
    "You’ll learn how to use a memory object to store and retrieve conversation history, tool usage, and other relevant information, grouped by session. This is a key step toward building agents that can hold a conversation or remember facts within a session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e57bd8",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "- Understand the difference between state and memory:\n",
    "    - State is local to a single run and is lost when the run ends.\n",
    "    - Memory persists across runs and sessions, allowing the agent to remember what happened before.\n",
    "- Use the provided ShortTermMemory class to manage session memory.\n",
    "- Implement an Agent class that:\n",
    "    - Accepts a session_id for each interaction.\n",
    "    - Stores each state in memory under the correct session.\n",
    "    - Retrieves and uses session history to provide context for new queries.\n",
    "- Demonstrate how the agent can continue a conversation across multiple invocations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa44fc7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "id": "08286c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:01.640730Z",
     "start_time": "2025-11-05T19:30:01.637617Z"
    }
   },
   "source": [
    "from typing import TypedDict, List, Optional, Union\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from lib.state_machine import StateMachine, Step, EntryPoint, Termination, Run\n",
    "from lib.llm import LLM\n",
    "from lib.messages import AIMessage, UserMessage, SystemMessage, ToolMessage, BaseMessage\n",
    "from lib.tooling import Tool, ToolCall, tool\n",
    "from lib.memory import ShortTermMemory"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "4002bf4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:02.254887Z",
     "start_time": "2025-11-05T19:30:02.251576Z"
    }
   },
   "source": [
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:04.347613Z",
     "start_time": "2025-11-05T19:30:04.337231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url = \"https://openai.vocareum.com/v1\"\n",
    ")"
   ],
   "id": "297216e16b9960de",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "2371134f",
   "metadata": {},
   "source": [
    "## Define a State Schema\n",
    "\n",
    "Create a TypedDict to represent the agent’s state, including fields for the user query, instructions, message history, any pending tool calls and the session_id."
   ]
  },
  {
   "cell_type": "code",
   "id": "d3ba21e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:05.501598Z",
     "start_time": "2025-11-05T19:30:05.499162Z"
    }
   },
   "source": [
    "class AgentState(TypedDict):\n",
    "    user_query: str  # The current user query being processed\n",
    "    instructions: str  # System instructions for the agent\n",
    "    messages: List[dict]  # List of conversation messages\n",
    "    current_tool_calls: Optional[List[ToolCall]]  # Current pending tool calls\n",
    "    session_id: str  # Session identifier for memory management\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "ebd4dc40",
   "metadata": {},
   "source": [
    "## Create your Agent with Memory"
   ]
  },
  {
   "cell_type": "code",
   "id": "9aef3110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:06.609191Z",
     "start_time": "2025-11-05T19:30:06.600902Z"
    }
   },
   "source": [
    "class MemoryAgent:\n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 instructions: str, \n",
    "                 tools: List[Tool] = None,\n",
    "                 temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize a MemoryAgent instance\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name/identifier of the LLM model to use\n",
    "            instructions: System instructions for the agent\n",
    "            tools: Optional list of tools available to the agent\n",
    "            temperature: Temperature parameter for LLM (default: 0.7)\n",
    "        \"\"\"\n",
    "        self.instructions = instructions\n",
    "        self.tools = tools if tools else []\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Initialize memory and state machine\n",
    "        self.memory = ShortTermMemory()\n",
    "        self.workflow = self._create_state_machine()\n",
    "\n",
    "    def _prepare_messages_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Prepare messages for LLM consumption\"\"\"\n",
    "        messages = state.get(\"messages\", [])\n",
    "        \n",
    "        # If no messages exist, start with system message\n",
    "        if not messages:\n",
    "            messages = [SystemMessage(content=state[\"instructions\"])]\n",
    "            \n",
    "        # Add the new user message\n",
    "        messages.append(UserMessage(content=state[\"user_query\"]))\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"session_id\": state[\"session_id\"]\n",
    "        }\n",
    "\n",
    "    def _llm_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Process the current state through the LLM\"\"\"\n",
    "        # Initialize LLM\n",
    "        llm = LLM(\n",
    "            model=self.model_name,\n",
    "            temperature=self.temperature,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        tool_calls = response.tool_calls if response.tool_calls else None\n",
    "\n",
    "        # Create AI message with content and tool calls\n",
    "        ai_message = AIMessage(content=response.content, tool_calls=tool_calls)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [ai_message],\n",
    "            \"current_tool_calls\": tool_calls,\n",
    "            \"session_id\": state[\"session_id\"]\n",
    "        }\n",
    "\n",
    "    def _tool_step(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Step logic: Execute any pending tool calls\"\"\"\n",
    "        tool_calls = state[\"current_tool_calls\"] or []\n",
    "        tool_messages = []\n",
    "        \n",
    "        for call in tool_calls:\n",
    "            # Access tool call data correctly\n",
    "            function_name = call.function.name\n",
    "            function_args = json.loads(call.function.arguments)\n",
    "            tool_call_id = call.id\n",
    "            # Find the matching tool\n",
    "            tool = next((t for t in self.tools if t.name == function_name), None)\n",
    "            if tool:\n",
    "                result = tool(**function_args)\n",
    "                tool_message = ToolMessage(\n",
    "                    content=json.dumps(result), \n",
    "                    tool_call_id=tool_call_id, \n",
    "                    name=function_name, \n",
    "                )\n",
    "                tool_messages.append(tool_message)\n",
    "        \n",
    "        # Clear tool calls and add results to messages\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + tool_messages,\n",
    "            \"current_tool_calls\": None,\n",
    "            \"session_id\": state[\"session_id\"]\n",
    "        }\n",
    "\n",
    "    def _create_state_machine(self) -> StateMachine[AgentState]:\n",
    "        \"\"\"Create the internal state machine for the agent\"\"\"\n",
    "        machine = StateMachine[AgentState](AgentState)\n",
    "        \n",
    "        # Create steps\n",
    "        entry = EntryPoint[AgentState]()\n",
    "        message_prep = Step[AgentState](\"message_prep\", self._prepare_messages_step)\n",
    "        llm_processor = Step[AgentState](\"llm_processor\", self._llm_step)\n",
    "        tool_executor = Step[AgentState](\"tool_executor\", self._tool_step)\n",
    "        termination = Termination[AgentState]()\n",
    "        \n",
    "        machine.add_steps([entry, message_prep, llm_processor, tool_executor, termination])\n",
    "        \n",
    "        # Add transitions\n",
    "        machine.connect(entry, message_prep)\n",
    "        machine.connect(message_prep, llm_processor)\n",
    "        \n",
    "        # Transition based on whether there are tool calls\n",
    "        def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:\n",
    "            \"\"\"Transition logic: Check if there are tool calls\"\"\"\n",
    "            if state.get(\"current_tool_calls\"):\n",
    "                return tool_executor\n",
    "            return termination\n",
    "        \n",
    "        machine.connect(llm_processor, [tool_executor, termination], check_tool_calls)\n",
    "        machine.connect(tool_executor, llm_processor)  # Go back to llm after tool execution\n",
    "        \n",
    "        return machine\n",
    "\n",
    "    def invoke(self, query: str, session_id: Optional[str] = None) -> Run:\n",
    "        \"\"\"\n",
    "        Run the agent on a query\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query to process\n",
    "            session_id: Optional session identifier (uses \"default\" if None)\n",
    "            \n",
    "        Returns:\n",
    "            The final run object after processing\n",
    "        \"\"\"\n",
    "        session_id = session_id or \"default\"\n",
    "\n",
    "        # Create session if it doesn't exist\n",
    "        self.memory.create_session(session_id)\n",
    "\n",
    "        # Get previous messages from last run if available\n",
    "        previous_messages = []\n",
    "        last_run: Run = self.memory.get_last_object(session_id)\n",
    "        if last_run:\n",
    "            last_state = last_run.get_final_state()\n",
    "            if last_state:\n",
    "                previous_messages = last_state[\"messages\"]\n",
    "\n",
    "        initial_state: AgentState = {\n",
    "            \"user_query\": query,\n",
    "            \"instructions\": self.instructions,\n",
    "            \"messages\": previous_messages,\n",
    "            \"current_tool_calls\": None,\n",
    "            \"session_id\": session_id,\n",
    "        }\n",
    "\n",
    "        run_object = self.workflow.run(initial_state)\n",
    "        \n",
    "        # Store the complete run object in memory\n",
    "        self.memory.add(run_object, session_id)\n",
    "        \n",
    "        return run_object\n",
    "\n",
    "    def get_session_runs(self, session_id: Optional[str] = None) -> List[Run]:\n",
    "        \"\"\"Get all Run objects for a session\n",
    "        \n",
    "        Args:\n",
    "            session_id: Optional session ID (uses \"default\" if None)\n",
    "            \n",
    "        Returns:\n",
    "            List of Run objects in the session\n",
    "        \"\"\"\n",
    "        return self.memory.get_all_objects(session_id)\n",
    "\n",
    "    def reset_session(self, session_id: Optional[str] = None):\n",
    "        \"\"\"Reset memory for a specific session\n",
    "        \n",
    "        Args:\n",
    "            session_id: Optional session to reset (uses \"default\" if None)\n",
    "        \"\"\"\n",
    "        self.memory.reset(session_id)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "d2cfcce5",
   "metadata": {},
   "source": [
    "## Define your tools and instantiate your Agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "7657a6f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:07.907671Z",
     "start_time": "2025-11-05T19:30:07.904875Z"
    }
   },
   "source": [
    "@tool\n",
    "def get_games(num_games:int=1, top:bool=True) -> str:\n",
    "    \"\"\"\n",
    "    Returns the top or bottom N games with highest or lowest scores.    \n",
    "    args:\n",
    "        num_games (int): Number of games to return (default is 1)\n",
    "        top (bool): If True, return top games, otherwise return bottom (default is True)\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98},\n",
    "        {\"Game\": \"Super Mario Odyssey\", \"Platform\": \"Switch\", \"Score\": 97},\n",
    "        {\"Game\": \"Metroid Prime\", \"Platform\": \"GameCube\", \"Score\": 97},\n",
    "        {\"Game\": \"Super Smash Bros. Brawl\", \"Platform\": \"Wii\", \"Score\": 93},\n",
    "        {\"Game\": \"Mario Kart 8 Deluxe\", \"Platform\": \"Switch\", \"Score\": 92},\n",
    "        {\"Game\": \"Fire Emblem: Awakening\", \"Platform\": \"3DS\", \"Score\": 92},\n",
    "        {\"Game\": \"Donkey Kong Country Returns\", \"Platform\": \"Wii\", \"Score\": 87},\n",
    "        {\"Game\": \"Luigi's Mansion 3\", \"Platform\": \"Switch\", \"Score\": 86},\n",
    "        {\"Game\": \"Pikmin 3\", \"Platform\": \"Wii U\", \"Score\": 85},\n",
    "        {\"Game\": \"Animal Crossing: New Leaf\", \"Platform\": \"3DS\", \"Score\": 88}\n",
    "    ]\n",
    "    # Sort the games list by Score\n",
    "    # If top is True, descending order\n",
    "    sorted_games = sorted(data, key=lambda x: x['Score'], reverse=top)\n",
    "    \n",
    "    # Return the N games\n",
    "    return sorted_games[:num_games]"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "9f67b873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:08.646376Z",
     "start_time": "2025-11-05T19:30:08.644026Z"
    }
   },
   "source": [
    "tools = [get_games]"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "c7ff665d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:09.394748Z",
     "start_time": "2025-11-05T19:30:09.392143Z"
    }
   },
   "source": [
    "agent = MemoryAgent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    instructions=\"You can bring insights about a game dataset based on users questions\",\n",
    "    tools=tools\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "c74d8aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:10.161390Z",
     "start_time": "2025-11-05T19:30:10.158606Z"
    }
   },
   "source": [
    "def print_messages(messages: List[BaseMessage]):\n",
    "    for m in messages:\n",
    "        print(f\" -> (role = {m.role}, content = {m.content}, tool_calls = {getattr(m, 'tool_calls', None)})\")"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "bf6168f5",
   "metadata": {},
   "source": [
    "## Run your Agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "89c9238c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T19:30:12.341981Z",
     "start_time": "2025-11-05T19:30:12.033763Z"
    }
   },
   "source": [
    "# First interaction in session \"games\"\n",
    "print(\"First interaction:\")\n",
    "run1 = agent.invoke(\"What's the best game in the dataset?\", \"games\")\n",
    "\n",
    "print(\"\\nMessages from run 1:\")\n",
    "messages = run1.get_final_state()[\"messages\"]\n",
    "print_messages(messages)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First interaction:\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: voc-1454**************************************6929. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAuthenticationError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# First interaction in session \"games\"\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mFirst interaction:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m run1 = \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWhat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43ms the best game in the dataset?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgames\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mMessages from run 1:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m messages = run1.get_final_state()[\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 151\u001B[39m, in \u001B[36mMemoryAgent.invoke\u001B[39m\u001B[34m(self, query, session_id)\u001B[39m\n\u001B[32m    141\u001B[39m         previous_messages = last_state[\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    143\u001B[39m initial_state: AgentState = {\n\u001B[32m    144\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33muser_query\u001B[39m\u001B[33m\"\u001B[39m: query,\n\u001B[32m    145\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33minstructions\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m.instructions,\n\u001B[32m   (...)\u001B[39m\u001B[32m    148\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33msession_id\u001B[39m\u001B[33m\"\u001B[39m: session_id,\n\u001B[32m    149\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m151\u001B[39m run_object = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mworkflow\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# Store the complete run object in memory\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28mself\u001B[39m.memory.add(run_object, session_id)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/3-building-agents/04-short-term-memory/lib/state_machine.py:206\u001B[39m, in \u001B[36mStateMachine.run\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m    203\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    205\u001B[39m \u001B[38;5;66;03m# Replace state entirely\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m state = \u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstate_schema\u001B[49m\u001B[43m)\u001B[49m  \n\u001B[32m    208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(step, EntryPoint):\n\u001B[32m    209\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[StateMachine] Starting: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_step_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/3-building-agents/04-short-term-memory/lib/state_machine.py:22\u001B[39m, in \u001B[36mStep.run\u001B[39m\u001B[34m(self, state, state_schema)\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: StateSchema, state_schema: Type[StateSchema]) -> StateSchema:\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlogic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m     \u001B[38;5;66;03m# Get expected fields from the TypedDict\u001B[39;00m\n\u001B[32m     24\u001B[39m     expected_fields = get_type_hints(state_schema)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 50\u001B[39m, in \u001B[36mMemoryAgent._llm_step\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;66;03m# Initialize LLM\u001B[39;00m\n\u001B[32m     44\u001B[39m llm = LLM(\n\u001B[32m     45\u001B[39m     model=\u001B[38;5;28mself\u001B[39m.model_name,\n\u001B[32m     46\u001B[39m     temperature=\u001B[38;5;28mself\u001B[39m.temperature,\n\u001B[32m     47\u001B[39m     tools=\u001B[38;5;28mself\u001B[39m.tools\n\u001B[32m     48\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m response = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m tool_calls = response.tool_calls \u001B[38;5;28;01mif\u001B[39;00m response.tool_calls \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     53\u001B[39m \u001B[38;5;66;03m# Create AI message with content and tool calls\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/3-building-agents/04-short-term-memory/lib/llm.py:63\u001B[39m, in \u001B[36mLLM.invoke\u001B[39m\u001B[34m(self, input, response_format)\u001B[39m\n\u001B[32m     61\u001B[39m     response = \u001B[38;5;28mself\u001B[39m.client.beta.chat.completions.parse(**payload)\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     64\u001B[39m choice = response.choices[\u001B[32m0\u001B[39m]\n\u001B[32m     65\u001B[39m message = choice.message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/_utils/_utils.py:287\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    285\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:925\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    882\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    883\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    884\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    922\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = NOT_GIVEN,\n\u001B[32m    923\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m    924\u001B[39m     validate_response_format(response_format)\n\u001B[32m--> \u001B[39m\u001B[32m925\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    938\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    939\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    940\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodalities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprediction\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweb_search_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/_base_client.py:1239\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1225\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1226\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1227\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1234\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1235\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1236\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1237\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1238\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1239\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/agentic-ai/.venv/lib/python3.13/site-packages/openai/_base_client.py:1034\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1031\u001B[39m             err.response.read()\n\u001B[32m   1033\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1034\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1036\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mAuthenticationError\u001B[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: voc-1454**************************************6929. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e6b006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second interaction (same session):\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "\n",
      "Messages from run 2:\n",
      " -> (role = system, content = You can bring insights about a game dataset based on users questions, tool_calls = None)\n",
      " -> (role = user, content = What's the best game in the dataset?, tool_calls = None)\n",
      " -> (role = assistant, content = None, tool_calls = [ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')])\n",
      " -> (role = tool, content = [{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}], tool_calls = None)\n",
      " -> (role = assistant, content = The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**., tool_calls = None)\n",
      " -> (role = user, content = And what was its score?, tool_calls = None)\n",
      " -> (role = assistant, content = The score of **The Legend of Zelda: Breath of the Wild** is **98**., tool_calls = None)\n"
     ]
    }
   ],
   "source": [
    "# Second interaction in same session\n",
    "print(\"\\nSecond interaction (same session):\")\n",
    "run2 = agent.invoke(\"And what was its score?\", \"games\")\n",
    "\n",
    "print(\"\\nMessages from run 2:\")\n",
    "messages = run2.get_final_state()[\"messages\"]\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66335d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New session interaction:\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "\n",
      "Messages from run 3:\n",
      " -> (role = system, content = You can bring insights about a game dataset based on users questions, tool_calls = None)\n",
      " -> (role = user, content = What's the worst game?, tool_calls = None)\n",
      " -> (role = assistant, content = None, tool_calls = [ChatCompletionMessageToolCall(id='call_k28lcnTDh43aJfp2fqf2ud8e', function=Function(arguments='{\"num_games\":1,\"top\":false}', name='get_games'), type='function')])\n",
      " -> (role = tool, content = [{\"Game\": \"Pikmin 3\", \"Platform\": \"Wii U\", \"Score\": 85}], tool_calls = None)\n",
      " -> (role = assistant, content = The worst game, based on the lowest score, is \"Pikmin 3\" for the Wii U, with a score of 85., tool_calls = None)\n"
     ]
    }
   ],
   "source": [
    "# New session\n",
    "print(\"\\nNew session interaction:\")\n",
    "run3 = agent.invoke(\"What's the worst game?\", \"other_session\")\n",
    "\n",
    "print(\"\\nMessages from run 3:\")\n",
    "messages = run3.get_final_state()[\"messages\"]\n",
    "print_messages(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160949c",
   "metadata": {},
   "source": [
    "## Check session histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fdabe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games session runs:\n",
      "\n",
      "# Run 1 {'run_id': '08203247-b0ba-4dd5-b2fc-280d653d7ab3', 'start_timestamp': '2025-05-12 03:06:05.323496', 'end_timestamp': '2025-05-12 03:06:08.289115', 'snapshot_counts': 5}\n",
      "Messages:\n",
      " -> (role = system, content = You can bring insights about a game dataset based on users questions, tool_calls = None)\n",
      " -> (role = user, content = What's the best game in the dataset?, tool_calls = None)\n",
      " -> (role = assistant, content = None, tool_calls = [ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')])\n",
      " -> (role = tool, content = [{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}], tool_calls = None)\n",
      " -> (role = assistant, content = The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**., tool_calls = None)\n",
      "\n",
      "# Run 2 {'run_id': '09f29ee6-f883-4df1-a228-409a73305ed4', 'start_timestamp': '2025-05-12 03:06:08.299878', 'end_timestamp': '2025-05-12 03:06:09.321775', 'snapshot_counts': 3}\n",
      "Messages:\n",
      " -> (role = system, content = You can bring insights about a game dataset based on users questions, tool_calls = None)\n",
      " -> (role = user, content = What's the best game in the dataset?, tool_calls = None)\n",
      " -> (role = assistant, content = None, tool_calls = [ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')])\n",
      " -> (role = tool, content = [{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}], tool_calls = None)\n",
      " -> (role = assistant, content = The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**., tool_calls = None)\n",
      " -> (role = user, content = And what was its score?, tool_calls = None)\n",
      " -> (role = assistant, content = The score of **The Legend of Zelda: Breath of the Wild** is **98**., tool_calls = None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Games session runs:\")\n",
    "runs = agent.get_session_runs(\"games\")\n",
    "for i, run_object in enumerate(runs, 1):\n",
    "    print(f\"\\n# Run {i}\", run_object.metadata)\n",
    "    print(\"Messages:\")\n",
    "    print_messages(run_object.get_final_state()[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dafe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games session snapshots:\n",
      "\n",
      "Run('08203247-b0ba-4dd5-b2fc-280d653d7ab3')\n",
      "-> Snapshot(e3a47265-bde3-4dd4-9583-8fe1487bc82c) @ [2025-05-12 03:06:05.323543]: __entry__.State({'user_query': \"What's the best game in the dataset?\", 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "-> Snapshot(a3769f17-639f-44db-8aa6-5395bb57d6df) @ [2025-05-12 03:06:05.323624]: message_prep.State({'user_query': \"What's the best game in the dataset?\", 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\")], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "-> Snapshot(97769972-e1b9-4335-b88b-85a39c699c18) @ [2025-05-12 03:06:06.579710]: llm_processor.State({'user_query': \"What's the best game in the dataset?\", 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\"), AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')])], 'current_tool_calls': [ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')], 'session_id': 'games'})\n",
      "-> Snapshot(28ad5596-752a-4111-b088-29c2d36fc66f) @ [2025-05-12 03:06:06.579834]: tool_executor.State({'user_query': \"What's the best game in the dataset?\", 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\"), AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')]), ToolMessage(role='tool', content='[{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}]', tool_call_id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', name='get_games')], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "-> Snapshot(151263ec-e52f-45e8-b521-d84eeb9b5ace) @ [2025-05-12 03:06:08.289051]: llm_processor.State({'user_query': \"What's the best game in the dataset?\", 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\"), AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')]), ToolMessage(role='tool', content='[{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}]', tool_call_id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', name='get_games'), AIMessage(role='assistant', content='The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**.', tool_calls=None)], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "\n",
      "\n",
      "Run('09f29ee6-f883-4df1-a228-409a73305ed4')\n",
      "-> Snapshot(c88e8ea9-e3c9-4a8d-b74f-b8fee3a298eb) @ [2025-05-12 03:06:08.300023]: __entry__.State({'user_query': 'And what was its score?', 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\"), AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')]), ToolMessage(role='tool', content='[{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}]', tool_call_id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', name='get_games'), AIMessage(role='assistant', content='The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**.', tool_calls=None)], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "-> Snapshot(0d56aa9c-1dbf-4b07-b2e6-cd49bdfe1435) @ [2025-05-12 03:06:08.300161]: message_prep.State({'user_query': 'And what was its score?', 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\"), AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')]), ToolMessage(role='tool', content='[{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}]', tool_call_id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', name='get_games'), AIMessage(role='assistant', content='The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**.', tool_calls=None), UserMessage(role='user', content='And what was its score?')], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "-> Snapshot(e749a6c4-f51e-4e71-8aa0-9d188abde6a1) @ [2025-05-12 03:06:09.321760]: llm_processor.State({'user_query': 'And what was its score?', 'instructions': 'You can bring insights about a game dataset based on users questions', 'messages': [SystemMessage(role='system', content='You can bring insights about a game dataset based on users questions'), UserMessage(role='user', content=\"What's the best game in the dataset?\"), AIMessage(role='assistant', content=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', function=Function(arguments='{\"num_games\":1,\"top\":true}', name='get_games'), type='function')]), ToolMessage(role='tool', content='[{\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98}]', tool_call_id='call_SLomFYOi1ZA5YFwoO0zPBCoQ', name='get_games'), AIMessage(role='assistant', content='The best game in the dataset is **The Legend of Zelda: Breath of the Wild**, available on the Switch, with a score of **98**.', tool_calls=None), UserMessage(role='user', content='And what was its score?'), AIMessage(role='assistant', content='The score of **The Legend of Zelda: Breath of the Wild** is **98**.', tool_calls=None)], 'current_tool_calls': None, 'session_id': 'games'})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Games session snapshots:\\n\")\n",
    "\n",
    "runs = agent.get_session_runs(\"games\")\n",
    "for run_object in runs:\n",
    "    print(run_object)\n",
    "    for snp in run_object.snapshots:\n",
    "        print(f\"-> {snp}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59054976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
