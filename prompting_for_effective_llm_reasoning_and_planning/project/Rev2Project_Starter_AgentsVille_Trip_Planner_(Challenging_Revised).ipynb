{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Project: AgentsVille Trip Planner\n",
        "#\n",
        "# Welcome to your final project! In this notebook, you'll build the \"AgentsVille Trip Planner,\"\n",
        "# an AI-powered assistant that helps users plan trips to the imaginary city of AgentsVille.\n",
        "# You will apply the prompting techniques and agentic reasoning concepts you've learned\n",
        "# throughout the course.\n",
        "\n",
        "# Project Goal:\n",
        "# 1. Generate an initial, detailed travel itinerary based on user preferences.\n",
        "# 2. Enhance this itinerary using an AI agent that can use (simulated) tools.\n",
        "\n",
        "# --- 0. Setup ---\n",
        "# Import necessary libraries and set up the OpenAI client and helper functions.\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from enum import Enum\n",
        "from typing import List, Dict, Any, Optional, Literal\n",
        "\n",
        "from openai import OpenAI\n",
        "# Ensure you have pydantic installed: pip install pydantic\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Attempt to import from project_lib, provide stubs if not found\n",
        "try:\n",
        "    from project_lib import (\n",
        "        TravelPreferences, TravelItinerary, Activity, DayPlan, PackingListItem, # Pydantic Models\n",
        "        available_tools, # Dictionary of tool schemas and functions\n",
        "        # Tool implementations are called via available_tools dictionary\n",
        "        LLMToolCall # Pydantic model for tool calls (if you choose to use it for internal validation)\n",
        "    )\n",
        "    print(\"Successfully imported from project_lib.py\")\n",
        "except ImportError:\n",
        "    print(\"Warning: project_lib.py not found or some components are missing. Using placeholder Pydantic models and functions.\\n\",\n",
        "          \"Please ensure project_lib.py is in the same directory as this notebook for full functionality.\")\n",
        "\n",
        "    # Define dummy Pydantic models if project_lib is not available\n",
        "    class TravelPreferences(BaseModel):\n",
        "        destination: str\n",
        "        duration_days: int\n",
        "        travel_style: List[str]\n",
        "        interests: List[str]\n",
        "        budget: Literal[\"budget\", \"mid-range\", \"luxury\"]\n",
        "        specific_requests: Optional[str] = None\n",
        "\n",
        "    class Activity(BaseModel):\n",
        "        time: str\n",
        "        description: str\n",
        "        estimated_cost: Optional[str] = None\n",
        "        details: Optional[str] = None\n",
        "\n",
        "    class DayPlan(BaseModel):\n",
        "        day_number: int\n",
        "        theme: Optional[str] = None\n",
        "        activities: List[Activity]\n",
        "\n",
        "    class PackingListItem(BaseModel):\n",
        "        item: str\n",
        "        quantity: Optional[int] = 1\n",
        "        notes: Optional[str] = None\n",
        "\n",
        "    class TravelItinerary(BaseModel):\n",
        "        trip_name: str\n",
        "        destination: str\n",
        "        duration_days: int\n",
        "        daily_plans: List[DayPlan]\n",
        "        suggested_packing_list: Optional[List[PackingListItem]] = None\n",
        "        overall_estimated_budget: Optional[str] = None\n",
        "        notes: Optional[str] = None\n",
        "\n",
        "    # This LLMToolCall is for validating the *structure* of what the LLM intends to call.\n",
        "    # The actual OpenAI response will have a slightly different structure for `tool_calls`.\n",
        "    class LLMToolCall(BaseModel):\n",
        "        name: str\n",
        "        arguments: Dict[str, Any]\n",
        "\n",
        "    # Dummy tool implementations (these would be in project_lib.py)\n",
        "    def get_weather_forecast(location: str, date: str) -> Dict[str, Any]:\n",
        "        print(f\"[Tool Stub] Called get_weather_forecast for {location} on {date}\")\n",
        "        return {\"forecast\": \"Sunny\", \"temperature\": \"22C\", \"location\": location, \"date\": date}\n",
        "\n",
        "    def search_activities(location: str, interests: List[str], date: Optional[str] = None) -> List[Dict[str, Any]]:\n",
        "        print(f\"[Tool Stub] Called search_activities for {location} with interests {interests} on {date}\")\n",
        "        return [{\"name\": \"Museum of Agentic Design\", \"description\": \"Explore the history of AI agents.\"}]\n",
        "\n",
        "    def find_restaurants(location: str, cuisine_type: Optional[str] = None, price_range: Optional[str] = None) -> List[Dict[str, Any]]:\n",
        "        print(f\"[Tool Stub] Called find_restaurants for {location}, cuisine {cuisine_type}, price {price_range}\")\n",
        "        return [{\"name\": \"The Prompt Cafe\", \"cuisine\": cuisine_type if cuisine_type else \"various\"}]\n",
        "\n",
        "    def book_hotel(location: str, check_in_date: str, check_out_date: str, preferences: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "        print(f\"[Tool Stub] Called book_hotel for {location} from {check_in_date} to {check_out_date}\")\n",
        "        return {\"booking_confirmation\": \"HOTEL-STUB-CONFIRMED\", \"hotel_name\": \"The Grand Agent Hotel\"}\n",
        "\n",
        "    available_tools = {\n",
        "        \"get_weather_forecast\": {\n",
        "            \"function\": get_weather_forecast,\n",
        "            \"description\": \"Get the weather forecast for a specific location and date.\",\n",
        "            \"parameters\": { # This is a JSON schema object\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g., San Francisco, CA\"},\n",
        "                    \"date\": {\"type\": \"string\", \"description\": \"The date for the forecast, in YYYY-MM-DD format.\"}\n",
        "                },\n",
        "                \"required\": [\"location\", \"date\"]\n",
        "            }\n",
        "        },\n",
        "        \"search_activities\": {\n",
        "            \"function\": search_activities,\n",
        "            \"description\": \"Searches for activities based on location, interests, and optionally a date.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"The location to search for activities.\"},\n",
        "                    \"interests\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"A list of interests to guide the activity search.\"},\n",
        "                    \"date\": {\"type\": \"string\", \"description\": \"Optional. The specific date for activities, in YYYY-MM-DD format.\"}\n",
        "                },\n",
        "                \"required\": [\"location\", \"interests\"]\n",
        "            }\n",
        "        },\n",
        "        \"find_restaurants\": {\n",
        "            \"function\": find_restaurants,\n",
        "            \"description\": \"Finds restaurants based on location, cuisine type, and price range.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"The location to search for restaurants.\"},\n",
        "                    \"cuisine_type\": {\"type\": \"string\", \"description\": \"Optional. The desired type of cuisine (e.g., Italian, Mexican).\"},\n",
        "                    \"price_range\": {\"type\": \"string\", \"description\": \"Optional. The desired price range (e.g., $, $$, $$$).\"}\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        },\n",
        "        \"book_hotel\": {\n",
        "            \"function\": book_hotel,\n",
        "            \"description\": \"Books a hotel based on location, dates, and preferences.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"The location for the hotel.\"},\n",
        "                    \"check_in_date\": {\"type\": \"string\", \"description\": \"The check-in date in YYYY-MM-DD format.\"},\n",
        "                    \"check_out_date\": {\"type\": \"string\", \"description\": \"The check-out date in YYYY-MM-DD format.\"},\n",
        "                    \"preferences\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"description\": \"Optional. Dictionary of preferences like room type, amenities.\",\n",
        "                        \"properties\": { # Properties of the 'preferences' object\n",
        "                             \"room_type\": {\"type\": \"string\", \"description\": \"e.g., King, Queen, Suite\"},\n",
        "                             \"amenities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"e.g., ['pool', 'gym']\"}\n",
        "                        }, # No required fields for preferences, making it truly optional\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\", \"check_in_date\", \"check_out_date\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# OpenAI Client Setup\n",
        "# TODO: Configure your OpenAI API Key\n",
        "# Replace \"**********\" with your actual Vocareum OpenAI API Key.\n",
        "# Alternatively, if your classroom workspace sets the OPENAI_API_KEY environment variable,\n",
        "# you can use `api_key=os.getenv(\"OPENAI_API_KEY\")`.\n",
        "\n",
        "try:\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://openai.vocareum.com/v1\",\n",
        "        api_key=\"**********\"  # TODO: Replace \"**********\" with your key\n",
        "    )\n",
        "    print(\"OpenAI client configured (key needs to be set by student).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring OpenAI client: {e}\")\n",
        "\n",
        "# Define helper functions\n",
        "class OpenAIModels(str, Enum):\n",
        "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
        "\n",
        "MODEL = OpenAIModels.GPT_41_MINI # Using a specific model for consistency\n",
        "\n",
        "def get_completion(\n",
        "    messages: List[Dict[str, Any]],\n",
        "    model: str = MODEL,\n",
        "    temperature: float = 0.2, # Lower temperature for more deterministic structured output\n",
        "    response_format: Optional[Dict[str, str]] = None,\n",
        "    tools: Optional[List[Dict[str, Any]]] = None, # For native tool calling\n",
        "    tool_choice: Optional[str] = None # For native tool calling (\"auto\" or {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}})\n",
        ") -> Optional[Any]: # Returns the OpenAI message object or content string\n",
        "    \"\"\"\n",
        "    Function to get a completion from the OpenAI API.\n",
        "    Can handle direct content responses or responses indicating tool calls.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        completion_params = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "        }\n",
        "        if response_format:\n",
        "            completion_params[\"response_format\"] = response_format\n",
        "        if tools:\n",
        "            completion_params[\"tools\"] = tools\n",
        "        if tool_choice:\n",
        "            completion_params[\"tool_choice\"] = tool_choice\n",
        "\n",
        "        response = client.chat.completions.create(**completion_params)\n",
        "        message = response.choices[0].message\n",
        "\n",
        "        # If the LLM decides to call a tool, message.tool_calls will be populated.\n",
        "        # Otherwise, message.content will have the text response.\n",
        "        return message\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling OpenAI API: {e}\")\n",
        "        return None\n",
        "\n",
        "def print_messages(messages: List[Dict[str, Any]]):\n",
        "    \"\"\"Helper function to print messages, including tool calls and tool results.\"\"\"\n",
        "    for msg_idx, message in enumerate(messages):\n",
        "        role = message.get('role', 'unknown').capitalize()\n",
        "        content = message.get('content')\n",
        "\n",
        "        display(Markdown(f\"--- Message {msg_idx} ---\"))\n",
        "        display(Markdown(f\"**{role}:**\"))\n",
        "\n",
        "        if content and isinstance(content, str): # Standard text content\n",
        "            display(Markdown(content))\n",
        "\n",
        "        # Handling OpenAI v1.x tool_calls attribute on assistant message\n",
        "        if message.get('role') == 'assistant' and message.get('tool_calls'):\n",
        "            display(Markdown(\"  *Assistant wants to call tools:*\"))\n",
        "            for tc in message['tool_calls']:\n",
        "                # tc is a ChatCompletionMessageToolCall object\n",
        "                function_info = tc.function\n",
        "                display(Markdown(f\"  - **Tool Call ID:** `{tc.id}`\"))\n",
        "                display(Markdown(f\"    - **Function to Call:** `{function_info.name}`\"))\n",
        "                display(Markdown(f\"    - **Arguments (JSON string):** `{function_info.arguments}`\"))\n",
        "\n",
        "        # Handling 'tool' role messages (results of tool execution)\n",
        "        if message.get('role') == 'tool':\n",
        "            display(Markdown(f\"  *Result for Tool Call ID `{message.get('tool_call_id')}` (Function: `{message.get('name')}`):*\"))\n",
        "            if content: # Content here is the result from the tool\n",
        "                 display(Markdown(f\"    `{str(content)}`\"))\n",
        "        # Removed the extra \"---\" print to avoid double lines with the new \"Message idx\" line.\n",
        "\n",
        "# --- 1. Defining User Preferences (Input) ---\n",
        "user_preferences_data = {\n",
        "    \"destination\": \"AgentsVille\",\n",
        "    \"duration_days\": 3,\n",
        "    \"travel_style\": [\"culture\", \"history\", \"local experiences\"],\n",
        "    \"interests\": [\"museums\", \"historical landmarks\", \"local markets\", \"street food\"],\n",
        "    \"budget\": \"mid-range\",\n",
        "    \"specific_requests\": \"I'd like to visit at least one major museum and try authentic local cuisine. I prefer using public transport or walking.\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    user_prefs = TravelPreferences(**user_preferences_data)\n",
        "    print(\"User Preferences (Validated):\")\n",
        "    print(user_prefs.model_dump_json(indent=2))\n",
        "except ValidationError as e:\n",
        "    print(f\"Error validating user preferences: {e}\")\n",
        "    user_prefs = None\n",
        "\n",
        "# --- 2. Initial Itinerary Generation (CoT & Structured Output) ---\n",
        "# Reference: TravelItinerary Pydantic Model (defined in project_lib.py or stub above)\n",
        "\n",
        "# TODO: Craft the system prompt for initial itinerary generation.\n",
        "# This prompt must instruct the LLM to act as a detailed travel planner for AgentsVille.\n",
        "# It needs to consider all aspects of the `user_prefs`.\n",
        "# The output MUST be a JSON object that validates against the `TravelItinerary` Pydantic model.\n",
        "# Remind it to ONLY output the JSON object.\n",
        "# Apply principles of Chain-of-Thought by asking for a comprehensive, well-reasoned plan.\n",
        "\n",
        "system_prompt_generate_itinerary = \"\"\"\n",
        "********** TODO: DEFINE YOUR SYSTEM PROMPT FOR ITINERARY GENERATION HERE **********\n",
        "Key elements to include:\n",
        "- Role: Expert Travel Planner for AgentsVille\n",
        "- Task: Create a comprehensive, day-by-day itinerary.\n",
        "- Input: User preferences (will be provided in user message).\n",
        "- Output: A single JSON object strictly conforming to the TravelItinerary schema.\n",
        "  (You might want to paste the Pydantic model's field names as a reminder of the structure here for the LLM,\n",
        "   but the core instruction is to match the schema it will be validated against.)\n",
        "- Guidance: Consider all preferences (duration, style, interests, budget, specific requests).\n",
        "           Plan logically. Include activities, packing list, budget estimate, notes.\n",
        "           Output ONLY the JSON. No other text.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_itinerary_request = f\"\"\"\n",
        "Please generate a travel itinerary based on the following preferences:\n",
        "{user_prefs.model_dump_json(indent=2) if user_prefs else \"User preferences not available.\"}\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\\\nGenerating initial itinerary...\")\n",
        "initial_itinerary: Optional[TravelItinerary] = None\n",
        "raw_llm_itinerary_json_str = None\n",
        "\n",
        "if user_prefs:\n",
        "    messages_itinerary = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt_generate_itinerary},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_itinerary_request}\n",
        "    ]\n",
        "    # Requesting JSON object output directly from the API\n",
        "    llm_message_for_itinerary = get_completion(messages_itinerary, response_format={\"type\": \"json_object\"})\n",
        "\n",
        "    if llm_message_for_itinerary and isinstance(llm_message_for_itinerary.content, str):\n",
        "        raw_llm_itinerary_json_str = llm_message_for_itinerary.content\n",
        "        try:\n",
        "            initial_itinerary_json = json.loads(raw_llm_itinerary_json_str)\n",
        "            initial_itinerary = TravelItinerary(**initial_itinerary_json)\n",
        "            print(\"\\\\nInitial Itinerary Generated and Validated Successfully!\")\n",
        "            display(Markdown(f\"### Trip Name: {initial_itinerary.trip_name}\"))\n",
        "            # For full details, you can print:\n",
        "            # print(initial_itinerary.model_dump_json(indent=2))\n",
        "        except (ValidationError, json.JSONDecodeError) as e:\n",
        "            print(f\"\\\\nError processing itinerary JSON: {e}\")\n",
        "            print(f\"\\\\nLLM Response was (string):\\\\n{raw_llm_itinerary_json_str}\")\n",
        "    else:\n",
        "        print(\"\\\\nFailed to generate initial itinerary (LLM response was not direct content or was None).\")\n",
        "else:\n",
        "    print(\"User preferences not loaded, cannot generate itinerary.\")\n",
        "\n",
        "\n",
        "# --- 3. Tool-Using Agent for Enhancements (ReAct) ---\n",
        "# Create an AI agent that uses tools to answer follow-up questions or enhance the itinerary.\n",
        "\n",
        "print(\"\\\\nSchema for 'search_activities' tool (from available_tools dictionary):\")\n",
        "if \"search_activities\" in available_tools:\n",
        "    print(json.dumps(available_tools['search_activities']['parameters'], indent=2))\n",
        "\n",
        "# TODO: Craft the system prompt for the ReAct agent.\n",
        "# This is a challenging prompt. You need to instruct the LLM on:\n",
        "# - Its role (AI Travel Assistant for AgentsVille).\n",
        "# - The context (it has an existing itinerary, needs to answer questions or enhance it).\n",
        "# - The THINK-ACT-OBSERVE cycle.\n",
        "#   - THINK: Plan, reason, decide on tool.\n",
        "#   - ACT: Output *only* a JSON list of tool calls if using tools (e.g., `[{\"name\": \"tool_name\", \"arguments\": {...}}]`),\n",
        "#          OR output `FINAL ANSWER:` followed by text if answering directly.\n",
        "# - The list of AVAILABLE TOOLS: Provide their names, descriptions, and parameter schemas\n",
        "#   (you'll need to format this from the `available_tools` dictionary).\n",
        "# - Clear EXAMPLES of both a tool-calling interaction and a direct final answer.\n",
        "\n",
        "system_prompt_react_agent = f\"\"\"\n",
        "********** TODO: DEFINE YOUR REACT AGENT SYSTEM PROMPT HERE **********\n",
        "Key sections to include:\n",
        "1. Role and Goal.\n",
        "2. Detailed instructions on the THINK-ACT-OBSERVE cycle.\n",
        "   - Emphasize the exact output format for ACT (tool calls as JSON list OR FINAL ANSWER: text).\n",
        "3. The `AVAILABLE TOOLS` section:\n",
        "   You should dynamically build this part of the prompt by iterating through the\n",
        "   `available_tools` dictionary and formatting each tool's name, description,\n",
        "   and parameters (JSON schema) clearly for the LLM.\n",
        "4. Clear examples of interaction flows (one with a tool call, one with a direct final answer).\n",
        "\"\"\"\n",
        "\n",
        "# --- Simulate ReAct Agent Interaction ---\n",
        "react_messages: List[Dict[str, Any]] = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt_react_agent}\n",
        "]\n",
        "\n",
        "if initial_itinerary:\n",
        "    current_itinerary_str = initial_itinerary.model_dump_json(indent=2)\n",
        "    follow_up_request = \"Can you find a unique local craft workshop for the afternoon of Day 2? Also, what will the weather be like on Day 1 of my trip, assuming Day 1 is 2025-10-26?\"\n",
        "\n",
        "    user_message_content_for_react = f\"\"\"Here is the current travel itinerary:\n",
        "```json\n",
        "{current_itinerary_str}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SRVCtsbf14_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My follow-up request is: {follow_up_request}\n",
        "\"\"\"\n",
        "    react_messages.append({\"role\": \"user\", \"content\": user_message_content_for_react})\n",
        "else:\n",
        "    print(\"Initial itinerary not available. Using a general request for ReAct agent.\")\n",
        "    react_messages.append({\"role\": \"user\", \"content\": \"Help me plan a 3-day cultural trip to AgentsVille for a mid-range budget. What are some must-see historical landmarks?\"})\n",
        "\n",
        "\n",
        "MAX_TURNS = 7\n",
        "final_answer_received = False\n",
        "\n",
        "print(\"\\\\n--- Starting ReAct agent interaction ---\")\n",
        "if len(react_messages) > 1:\n",
        "    print_messages([react_messages[-1]])\n",
        "else:\n",
        "    print(\"Warning: No initial user message for ReAct agent beyond system prompt.\")\n",
        "\n",
        "\n",
        "for turn in range(MAX_TURNS):\n",
        "    if len(react_messages) <= 1 : # Should not happen if user message was added\n",
        "        print(\"Stopping ReAct loop: No user query to process.\")\n",
        "        break\n",
        "            \n",
        "    print(f\"\\\\n--- Agent Turn {turn + 1} ---\")\n",
        "    \n",
        "    # Get LLM's response (this is an OpenAI Message object)\n",
        "    llm_response_message = get_completion(react_messages)\n",
        "\n",
        "    if not llm_response_message:\n",
        "        print(\"Agent did not provide a response. Ending interaction.\")\n",
        "        break\n",
        "    \n",
        "    # The llm_response_message from OpenAI v1.x+ client contains `content` and `tool_calls`.\n",
        "    # Add the assistant's entire message object to history.\n",
        "    # The `tool_calls` attribute will be a list of ChatCompletionMessageToolCall objects.\n",
        "    # We need to convert this to the dict format if we were to send it back to older API versions,\n",
        "    # but for OpenAI v1.x, we can append the message object directly if the API expects it.\n",
        "    # For clarity and explicit structure in `react_messages`, we'll construct a dict.\n",
        "    \n",
        "    assistant_response_for_history = {\"role\": \"assistant\"}\n",
        "    if llm_response_message.content:\n",
        "        assistant_response_for_history[\"content\"] = llm_response_message.content\n",
        "    if llm_response_message.tool_calls:\n",
        "        assistant_response_for_history[\"tool_calls\"] = [\n",
        "            {\"id\": tc.id, \"type\": tc.type, \"function\": {\"name\": tc.function.name, \"arguments\": tc.function.arguments}}\n",
        "            for tc in llm_response_message.tool_calls\n",
        "        ]\n",
        "    \n",
        "    react_messages.append(assistant_response_for_history)\n",
        "    print_messages([assistant_response_for_history]) # Display AI's full response (THINK/ACT part)\n",
        "\n",
        "    # TODO: Implement the ReAct Loop Logic.\n",
        "    # This is where you will apply the skills learned in the ReAct exercise.\n",
        "    #\n",
        "    # 1. Check for \"FINAL ANSWER:\" in `llm_response_message.content`.\n",
        "    #    If found, set `final_answer_received = True` and `break`.\n",
        "    #\n",
        "    # 2. If `llm_response_message.tool_calls` is present:\n",
        "    #    - This means the LLM wants to call one or more tools.\n",
        "    #    - Iterate through each `tool_call` in `llm_response_message.tool_calls`.\n",
        "    #        - Get the `tool_name` (from `tool_call.function.name`).\n",
        "    #        - Get the `tool_id` (from `tool_call.id`).\n",
        "    #        - Get the `tool_arguments_str` (from `tool_call.function.arguments`, which is a JSON string).\n",
        "    #        - Parse `tool_arguments_str` into a Python dictionary (`json.loads()`).\n",
        "    #        - Look up `tool_name` in your `available_tools` to get the actual Python tool function.\n",
        "    #        - Execute the tool: `tool_result = tool_function(**parsed_arguments)`. Handle potential errors.\n",
        "    #        - Convert `tool_result` back to a JSON string.\n",
        "    #        - Construct a 'tool' role message to append to `react_messages`:\n",
        "    #          `{\"role\": \"tool\", \"tool_call_id\": tool_id, \"name\": tool_name, \"content\": tool_result_json_string}`\n",
        "    #          (The 'name' field in the tool role message is for matching the function name, as per OpenAI spec).\n",
        "    #\n",
        "    # 3. If it's not a final answer and not a tool call (i.e., `llm_response_message.content` is text but no \"FINAL ANSWER:\"):\n",
        "    #    The LLM might be asking for clarification or made an error. For this project,\n",
        "    #    you can append a generic \"OBSERVATION: Your response was not a FINAL ANSWER or a tool call. Please proceed.\"\n",
        "    #    message from the 'user' role to `react_messages` to prompt the LLM again.\n",
        "\n",
        "    # --- START TODO: Implement ReAct Loop Logic (Parsing ACT, Calling Tools, Appending Observations) ---\n",
        "    \n",
        "    # Student needs to fill this logic based on the ReAct exercise.\n",
        "    # Key parts:\n",
        "    # - Check for \"FINAL ANSWER:\" in llm_response_message.content\n",
        "    # - If not, check for llm_response_message.tool_calls\n",
        "    #   - If tool_calls exist:\n",
        "    #     - For each tool_call:\n",
        "    #       - Get tool_name, tool_id, arguments_json_string\n",
        "    #       - Parse arguments_json_string to a dict\n",
        "    #       - Find the corresponding python function from available_tools\n",
        "    #       - Execute the python function with the arguments\n",
        "    #       - Create a 'tool' role message with tool_call_id, name, and stringified result\n",
        "    #       - Append this 'tool' message to react_messages\n",
        "    # - If neither FINAL ANSWER nor tool_calls, append a generic user observation to try and guide the LLM.\n",
        "\n",
        "    # Placeholder for student implementation:\n",
        "    if llm_response_message.content and \"FINAL ANSWER:\" in llm_response_message.content:\n",
        "        final_answer_received = True\n",
        "        print(\"\\\\nAgent provided FINAL ANSWER.\")\n",
        "        break\n",
        "    elif llm_response_message.tool_calls:\n",
        "        # This section is critical for the student to implement correctly\n",
        "        # based on their understanding of the ReAct exercise and OpenAI tool calling.\n",
        "        print(\"  (Student TODO: Implement tool call execution and observation appending here)\")\n",
        "        # As a simple stub for now, let's add a generic observation if tools were called but not handled\n",
        "        # This part MUST be replaced by the student with actual tool execution logic.\n",
        "        if not final_answer_received: # Only if we haven't already decided to break\n",
        "             for tc_obj in llm_response_message.tool_calls:\n",
        "                # This is a placeholder observation. Student needs to call the actual tool.\n",
        "                stub_tool_observation = {\n",
        "                    \"role\": \"tool\",\n",
        "                    \"tool_call_id\": tc_obj.id,\n",
        "                    \"name\": tc_obj.function.name,\n",
        "                    \"content\": json.dumps({\"status\": \"success\", \"message\": f\"Tool {tc_obj.function.name} called (student needs to implement actual execution).\"})\n",
        "                }\n",
        "                react_messages.append(stub_tool_observation)\n",
        "                print_messages([stub_tool_observation])\n",
        "\n",
        "    else: # No FINAL ANSWER and no tool_calls\n",
        "        print(\"  Agent did not call a tool or provide a final answer. Adding a generic observation.\")\n",
        "        generic_observation = {\"role\": \"user\", \"content\": \"OBSERVATION: Your response did not include an 'ACT:' section with tool calls or a 'FINAL ANSWER:'. Please either call a tool or provide a final answer.\"}\n",
        "        react_messages.append(generic_observation)\n",
        "        print_messages([generic_observation])\n",
        "\n",
        "    # --- END TODO: Implement ReAct Loop Logic ---\n",
        "\n",
        "if not final_answer_received:\n",
        "    print(f\"\\\\nAgent did not provide a final answer after {MAX_TURNS} turns.\")\n",
        "\n",
        "\n",
        "# --- 4. Final Output and Reflection ---\n",
        "print(\"\\\\n--- Full ReAct Agent Conversation History (if run) ---\")\n",
        "if initial_itinerary or len(react_messages) > 1 :\n",
        "    print_messages(react_messages)\n",
        "\n",
        "# TODO: If the agent modified the itinerary or provided specific information in its FINAL ANSWER,\n",
        "# you might want to parse and display that here.\n",
        "\n",
        "# --- Reflection Questions: ---\n",
        "# (Keep the reflection questions as provided in the original starter)\n",
        "\n",
        "# --- Congratulations! ---\n",
        "# (Keep the congratulations message as provided)\n",
        "\n",
        "  ```"
      ],
      "metadata": {
        "id": "hseN63Ia14_d"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}